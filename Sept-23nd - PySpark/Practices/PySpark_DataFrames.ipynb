{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Install and Initialize PySpark\n",
        "We will now set up **PySpark** to work with product and order data.\n",
        "\n",
        "1. **Install PySpark**  \n",
        "   - Use `!pip install pyspark` since PySpark is not pre-installed in Google Colab.  \n",
        "\n",
        "2. **Create a SparkSession**  \n",
        "   - `SparkSession` is the entry point for using PySpark.  \n",
        "   - We set the application name as `\"Product-Order-Example\"`.  \n",
        "   - Once created, the `spark` object will let us work with DataFrames and SQL queries.\n"
      ],
      "metadata": {
        "id": "HpJPS8qgtcqV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByRpOwMfsdSU",
        "outputId": "2b79e00e-9f19-48d7-9a16-d82744eb2f0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Product-Order-Example\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Product and Order DataFrames\n",
        "We will now create two sample datasets in PySpark:\n",
        "\n",
        "1. **Product Data**  \n",
        "   - Contains details like `product_id`, `name`, `category`, and `price`.  \n",
        "   - Example: `(101, \"Laptop\", \"Electronics\", 55000)`.  \n",
        "\n",
        "2. **Order Data**  \n",
        "   - Contains details like `order_id`, `product_id`, `quantity`, and `customer`.  \n",
        "   - Example: `(201, 101, 2, \"Rahul Sharma\")`.  \n",
        "   - Notice one record has a `product_id` (106) that does not exist in the product catalog → simulates invalid data.  \n",
        "\n",
        "3. Convert both lists into PySpark **DataFrames** using `spark.createDataFrame()`.  \n",
        "\n",
        "4. Use `.show()` to display the DataFrames.\n"
      ],
      "metadata": {
        "id": "uIWGnEgvtf-D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Product data\n",
        "product_data = [\n",
        "    (101, \"Laptop\", \"Electronics\", 55000),\n",
        "    (102, \"Mobile Phone\", \"Electronics\", 25000),\n",
        "    (103, \"Chair\", \"Furniture\", 5000),\n",
        "    (104, \"Book\", \"Stationery\", 300),\n",
        "    (105, \"Headphones\", \"Electronics\", 3000)\n",
        "]\n",
        "\n",
        "product_cols = [\"product_id\", \"name\", \"category\", \"price\"]\n",
        "product_df = spark.createDataFrame(product_data, product_cols)\n",
        "\n",
        "# Order data\n",
        "order_data = [\n",
        "    (201, 101, 2, \"Rahul Sharma\"),\n",
        "    (202, 102, 1, \"Priya Singh\"),\n",
        "    (203, 103, 4, \"Aman Kumar\"),\n",
        "    (204, 104, 10, \"Sneha Reddy\"),\n",
        "    (205, 101, 1, \"Arjun Mehta\"),\n",
        "    (206, 105, 3, \"Rahul Sharma\"),\n",
        "    (207, 106, 1, \"Ghost Customer\")  # Order with product not in catalog\n",
        "]\n",
        "\n",
        "order_cols = [\"order_id\", \"product_id\", \"quantity\", \"customer\"]\n",
        "order_df = spark.createDataFrame(order_data, order_cols)\n",
        "\n",
        "# Show both\n",
        "product_df.show()\n",
        "order_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4KPWRZKzs8bY",
        "outputId": "8a0749dc-2e76-41c5-c859-e61a7439915e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-----------+-----+\n",
            "|product_id|        name|   category|price|\n",
            "+----------+------------+-----------+-----+\n",
            "|       101|      Laptop|Electronics|55000|\n",
            "|       102|Mobile Phone|Electronics|25000|\n",
            "|       103|       Chair|  Furniture| 5000|\n",
            "|       104|        Book| Stationery|  300|\n",
            "|       105|  Headphones|Electronics| 3000|\n",
            "+----------+------------+-----------+-----+\n",
            "\n",
            "+--------+----------+--------+--------------+\n",
            "|order_id|product_id|quantity|      customer|\n",
            "+--------+----------+--------+--------------+\n",
            "|     201|       101|       2|  Rahul Sharma|\n",
            "|     202|       102|       1|   Priya Singh|\n",
            "|     203|       103|       4|    Aman Kumar|\n",
            "|     204|       104|      10|   Sneha Reddy|\n",
            "|     205|       101|       1|   Arjun Mehta|\n",
            "|     206|       105|       3|  Rahul Sharma|\n",
            "|     207|       106|       1|Ghost Customer|\n",
            "+--------+----------+--------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic Operations on Product DataFrame\n",
        "We can perform common DataFrame operations in PySpark:\n",
        "\n",
        "1. **Select specific columns**  \n",
        "   - `product_df.select(\"name\", \"price\")` → shows only product name and price.  \n",
        "\n",
        "2. **Filter rows**  \n",
        "   - `product_df.filter(product_df[\"price\"] > 10000)` → returns products with price greater than 10,000.  \n",
        "\n",
        "3. **Sort / Order by column**  \n",
        "   - `product_df.orderBy(product_df[\"price\"].desc())` → sorts products in descending order of price.\n"
      ],
      "metadata": {
        "id": "aFPoamNeuNTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select specific columns\n",
        "product_df.select(\"name\", \"price\").show()\n",
        "\n",
        "#Filter products with price > 10,000\n",
        "product_df.filter(product_df[\"price\"] > 10000).show()\n",
        "\n",
        "#Order products by price descending\n",
        "product_df.orderBy (product_df[\"price\"].desc()).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0ghj6t0uDLQ",
        "outputId": "fd222c0a-ba3b-4a0b-8c60-051a3f05fd75"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-----+\n",
            "|        name|price|\n",
            "+------------+-----+\n",
            "|      Laptop|55000|\n",
            "|Mobile Phone|25000|\n",
            "|       Chair| 5000|\n",
            "|        Book|  300|\n",
            "|  Headphones| 3000|\n",
            "+------------+-----+\n",
            "\n",
            "+----------+------------+-----------+-----+\n",
            "|product_id|        name|   category|price|\n",
            "+----------+------------+-----------+-----+\n",
            "|       101|      Laptop|Electronics|55000|\n",
            "|       102|Mobile Phone|Electronics|25000|\n",
            "+----------+------------+-----------+-----+\n",
            "\n",
            "+----------+------------+-----------+-----+\n",
            "|product_id|        name|   category|price|\n",
            "+----------+------------+-----------+-----+\n",
            "|       101|      Laptop|Electronics|55000|\n",
            "|       102|Mobile Phone|Electronics|25000|\n",
            "|       103|       Chair|  Furniture| 5000|\n",
            "|       105|  Headphones|Electronics| 3000|\n",
            "|       104|        Book| Stationery|  300|\n",
            "+----------+------------+-----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GroupBy and Aggregations in PySpark\n",
        "We can use **`groupBy()`** along with aggregation functions to analyze the data:\n",
        "\n",
        "1. **Total quantity ordered per product**  \n",
        "   - `order_df.groupBy(\"product_id\").sum(\"quantity\")`  \n",
        "   - Groups orders by product ID and calculates the total quantity ordered.  \n",
        "\n",
        "2. **Count of orders per customer**  \n",
        "   - `order_df.groupBy(\"customer\").count()`  \n",
        "   - Shows how many orders each customer has placed.  \n",
        "\n",
        "3. **Average price per category**  \n",
        "   - `product_df.groupBy(\"category\").avg(\"price\")`  \n",
        "   - Groups products by category and computes the average price within each category.\n"
      ],
      "metadata": {
        "id": "-6Mewtzo0bim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Total quantity ordered per product\n",
        "order_df.groupBy(\"product_id\").sum(\"quantity\").show()\n",
        "\n",
        "# Count of orders per customer\n",
        "order_df.groupBy(\"customer\").count().show()\n",
        "\n",
        "# Average price per category\n",
        "product_df.groupBy(\"category\").avg(\"price\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQdrerM20OPr",
        "outputId": "3aec0029-cade-45b3-ac80-41ab85a98736"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "|product_id|sum(quantity)|\n",
            "+----------+-------------+\n",
            "|       103|            4|\n",
            "|       101|            3|\n",
            "|       102|            1|\n",
            "|       104|           10|\n",
            "|       106|            1|\n",
            "|       105|            3|\n",
            "+----------+-------------+\n",
            "\n",
            "+--------------+-----+\n",
            "|      customer|count|\n",
            "+--------------+-----+\n",
            "|    Aman Kumar|    1|\n",
            "|  Rahul Sharma|    2|\n",
            "|   Priya Singh|    1|\n",
            "|   Arjun Mehta|    1|\n",
            "|Ghost Customer|    1|\n",
            "|   Sneha Reddy|    1|\n",
            "+--------------+-----+\n",
            "\n",
            "+-----------+------------------+\n",
            "|   category|        avg(price)|\n",
            "+-----------+------------------+\n",
            "|Electronics|27666.666666666668|\n",
            "| Stationery|             300.0|\n",
            "|  Furniture|            5000.0|\n",
            "+-----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Joins in PySpark\n",
        "\n",
        "We can combine **orders** and **products** using different types of joins:\n",
        "\n",
        "1. **Inner Join**  \n",
        "   - Returns only the matching rows (orders that have valid products).  \n",
        "\n",
        "2. **Left Join**  \n",
        "   - Returns all rows from the left table (`orders`), and matches from the right (`products`).  \n",
        "   - If a product is not found, its details will be `null`.  \n",
        "\n",
        "3. **Right Join**  \n",
        "   - Returns all rows from the right table (`products`), even if there are no matching orders.  \n",
        "   - Useful to see which products were never ordered.\n"
      ],
      "metadata": {
        "id": "xgHgDokb4Jfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Inner Join: Orders with product details\n",
        "order_df.join(product_df, order_df.product_id == product_df.product_id, \"inner\").show()\n",
        "\n",
        "# Left Join: All orders, even if product not found\n",
        "order_df.join(product_df, order_df.product_id == product_df.product_id, \"left\").show()\n",
        "\n",
        "#Right Join: All products, even if never ordered\n",
        "order_df.join(product_df, order_df.product_id == product_df.product_id, \"right\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD9wQkuD27Fs",
        "outputId": "a0fc8245-c5ce-4498-bf5c-eba1c3fc96f0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+--------+------------+----------+------------+-----------+-----+\n",
            "|order_id|product_id|quantity|    customer|product_id|        name|   category|price|\n",
            "+--------+----------+--------+------------+----------+------------+-----------+-----+\n",
            "|     201|       101|       2|Rahul Sharma|       101|      Laptop|Electronics|55000|\n",
            "|     205|       101|       1| Arjun Mehta|       101|      Laptop|Electronics|55000|\n",
            "|     202|       102|       1| Priya Singh|       102|Mobile Phone|Electronics|25000|\n",
            "|     203|       103|       4|  Aman Kumar|       103|       Chair|  Furniture| 5000|\n",
            "|     204|       104|      10| Sneha Reddy|       104|        Book| Stationery|  300|\n",
            "|     206|       105|       3|Rahul Sharma|       105|  Headphones|Electronics| 3000|\n",
            "+--------+----------+--------+------------+----------+------------+-----------+-----+\n",
            "\n",
            "+--------+----------+--------+--------------+----------+------------+-----------+-----+\n",
            "|order_id|product_id|quantity|      customer|product_id|        name|   category|price|\n",
            "+--------+----------+--------+--------------+----------+------------+-----------+-----+\n",
            "|     203|       103|       4|    Aman Kumar|       103|       Chair|  Furniture| 5000|\n",
            "|     201|       101|       2|  Rahul Sharma|       101|      Laptop|Electronics|55000|\n",
            "|     202|       102|       1|   Priya Singh|       102|Mobile Phone|Electronics|25000|\n",
            "|     204|       104|      10|   Sneha Reddy|       104|        Book| Stationery|  300|\n",
            "|     207|       106|       1|Ghost Customer|      NULL|        NULL|       NULL| NULL|\n",
            "|     206|       105|       3|  Rahul Sharma|       105|  Headphones|Electronics| 3000|\n",
            "|     205|       101|       1|   Arjun Mehta|       101|      Laptop|Electronics|55000|\n",
            "+--------+----------+--------+--------------+----------+------------+-----------+-----+\n",
            "\n",
            "+--------+----------+--------+------------+----------+------------+-----------+-----+\n",
            "|order_id|product_id|quantity|    customer|product_id|        name|   category|price|\n",
            "+--------+----------+--------+------------+----------+------------+-----------+-----+\n",
            "|     205|       101|       1| Arjun Mehta|       101|      Laptop|Electronics|55000|\n",
            "|     201|       101|       2|Rahul Sharma|       101|      Laptop|Electronics|55000|\n",
            "|     202|       102|       1| Priya Singh|       102|Mobile Phone|Electronics|25000|\n",
            "|     203|       103|       4|  Aman Kumar|       103|       Chair|  Furniture| 5000|\n",
            "|     204|       104|      10| Sneha Reddy|       104|        Book| Stationery|  300|\n",
            "|     206|       105|       3|Rahul Sharma|       105|  Headphones|Electronics| 3000|\n",
            "+--------+----------+--------+------------+----------+------------+-----------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PySpark SQL Queries\n",
        "\n",
        "- **Register Temporary Views**  \n",
        "  - `product_df` and `order_df` are registered as temporary views using `createOrReplaceTempView`.  \n",
        "  - This allows us to run SQL queries directly on these DataFrames.\n",
        "\n",
        "- **Total Revenue per Product**  \n",
        "  - Joins the `orders` and `products` tables on `product_id`.  \n",
        "  - Multiplies `quantity` by `price` for each order.  \n",
        "  - Sums the results to calculate the total revenue for each product.  \n",
        "  - Groups by `product_id` and product `name` to get revenue individually for each product.\n",
        "\n",
        "- **Top 2 Customers by Total Quantity Ordered**  \n",
        "  - Groups orders by `customer`.  \n",
        "  - Sums the `quantity` for each customer.  \n",
        "  - Sorts the results in descending order of total quantity.  \n",
        "  - Limits the output to the top 2 customers.\n"
      ],
      "metadata": {
        "id": "IjkNH9F-_ASS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register as temp views\n",
        "product_df.createOrReplaceTempView(\"products\")\n",
        "order_df.createOrReplaceTempView(\"orders\")\n",
        "\n",
        "#Query: Total revenue per product\n",
        "spark.sql(\"\"\"\n",
        "SELECT o.product_id, p.name, SUM(o.quantity * p.price) AS total_revenue\n",
        "FROM orders o\n",
        "JOIN products p ON o.product_id = p.product_id\n",
        "GROUP BY o.product_id, p.name\n",
        "\"\"\").show()\n",
        "\n",
        "#Query: Top 2 customers by total quantity\n",
        "spark.sql(\"\"\"\n",
        "SELECT customer, SUM(quantity) AS total_qty\n",
        "FROM orders\n",
        "GROUP BY customer\n",
        "ORDER BY total_qty DESC\n",
        "LIMIT 2\n",
        "\"\"\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oFNrQKQ8D2n",
        "outputId": "90a6e805-95c2-449a-f9b6-7bfcb5fab007"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+-------------+\n",
            "|product_id|        name|total_revenue|\n",
            "+----------+------------+-------------+\n",
            "|       101|      Laptop|       165000|\n",
            "|       102|Mobile Phone|        25000|\n",
            "|       103|       Chair|        20000|\n",
            "|       104|        Book|         3000|\n",
            "|       105|  Headphones|         9000|\n",
            "+----------+------------+-------------+\n",
            "\n",
            "+------------+---------+\n",
            "|    customer|total_qty|\n",
            "+------------+---------+\n",
            "| Sneha Reddy|       10|\n",
            "|Rahul Sharma|        5|\n",
            "+------------+---------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}